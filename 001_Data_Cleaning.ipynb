{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beijing Air-Quality Time Series Project\n",
    "### Data Cleaning Notebook\n",
    "\n",
    "by Dolci Sanders and Paul Torres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "\n",
    "Our Data is provided by the UCI Machine Learning Repository\n",
    "Beijing Multi-Site Air-Quality Data\n",
    "\n",
    "Once Read in, we will look at the head and convert our time into data time. \n",
    "\n",
    "We have 12 data sets, one from each reporting site, to concatenate together to get the whole picture of Beijing's Air Quality. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.read_csv('DATA/PRSA_Data_Tiantan_20130301-20170228.csv', index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2013, 2014, 2015, 2016, 2017])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.year.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Time Formatting\n",
    "Crucial to time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time['Date'] = pd.to_datetime(time[['year','month','day','hour']])\n",
    "time = time.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35064, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Nan Values\n",
    "After looking at the data, we wanted to look at null values and figure out what to do with these. \n",
    "\n",
    "We first calculated the mean and media for all of the values. These values varied wildly and we were concerned they would have a negative effect on the predictions. \n",
    "\n",
    "However, upon further research, other methods have proven more effective in inputing the data for time series such as Interpolating Time, however because we have hourly time this was not the best method. We elected to try imputing using the InterpolateLinear method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No            0\n",
       "year          0\n",
       "month         0\n",
       "day           0\n",
       "hour          0\n",
       "PM2.5       677\n",
       "PM10        597\n",
       "SO2        1118\n",
       "NO2         744\n",
       "CO         1126\n",
       "O3          843\n",
       "TEMP         20\n",
       "PRES         20\n",
       "DEWP         20\n",
       "RAIN         20\n",
       "wd           78\n",
       "WSPM         14\n",
       "station       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = time.interpolate(method='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PM2.5 (TARGET VARIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total PM2.5 Missing Values:  0\n",
      "mean:  82.03309662331738\n",
      "median:  58.0\n"
     ]
    }
   ],
   "source": [
    "print('total PM2.5 Missing Values: ', time['PM2.5'].isna().sum())\n",
    "print('mean: ', time['PM2.5'].mean())\n",
    "print('median: ', time['PM2.5'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PM10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total PM10 Missing Values:  0\n",
      "mean:  106.53707648870636\n",
      "median:  85.0\n"
     ]
    }
   ],
   "source": [
    "print('total PM10 Missing Values: ', time['PM10'].isna().sum())\n",
    "print('mean: ', time['PM10'].mean())\n",
    "print('median: ', time['PM10'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total SO2 Missing Values:  0\n",
      "mean:  14.510017738991555\n",
      "median:  7.0\n"
     ]
    }
   ],
   "source": [
    "print('total SO2 Missing Values: ', time['SO2'].isna().sum())\n",
    "print('mean: ', time['SO2'].mean())\n",
    "print('median: ', time['SO2'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total NO2 Missing Values:  0\n",
      "mean:  53.25882833532967\n",
      "median:  47.0\n"
     ]
    }
   ],
   "source": [
    "print('total NO2 Missing Values: ', time['NO2'].isna().sum())\n",
    "print('mean: ', time['NO2'].mean())\n",
    "print('median: ', time['NO2'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total CO Missing Values:  0\n",
      "mean:  1305.3332620351357\n",
      "median:  900.0\n"
     ]
    }
   ],
   "source": [
    "print('total CO Missing Values: ', time['CO'].isna().sum())\n",
    "print('mean: ', time['CO'].mean())\n",
    "print('median: ', time['CO'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total O3 Missing Values:  0\n",
      "mean:  56.14807717887289\n",
      "median:  40.0\n"
     ]
    }
   ],
   "source": [
    "print('total O3 Missing Values: ', time['O3'].isna().sum())\n",
    "print('mean: ', time['O3'].mean())\n",
    "print('median: ', time['O3'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total TEMP Missing Values:  0\n",
      "mean:  13.668249517775271\n",
      "median:  14.6\n"
     ]
    }
   ],
   "source": [
    "print('total TEMP Missing Values: ', time['TEMP'].isna().sum())\n",
    "print('mean: ', time['TEMP'].mean())\n",
    "print('median: ', time['TEMP'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total PRES Missing Values:  0\n",
      "mean:  1012.5518711023906\n",
      "median:  1012.2\n"
     ]
    }
   ],
   "source": [
    "print('total PRES Missing Values: ', time['PRES'].isna().sum())\n",
    "print('mean: ', time['PRES'].mean())\n",
    "print('median: ', time['PRES'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total DEWP Missing Values:  0\n",
      "mean:  2.4451260552133287\n",
      "median:  3.0\n"
     ]
    }
   ],
   "source": [
    "print('total DEWP Missing Values: ', time['DEWP'].isna().sum())\n",
    "print('mean: ', time['DEWP'].mean())\n",
    "print('median: ', time['DEWP'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total RAIN Missing Values:  0\n",
      "mean:  0.06398300250969628\n",
      "median:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('total RAIN Missing Values: ', time['RAIN'].isna().sum())\n",
    "print('mean: ', time['RAIN'].mean())\n",
    "print('median: ', time['RAIN'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wd\n",
    "\n",
    "wd is wind direction and is not an integer. We will forward fill the missing values. As wind direction does not change rapidly. --A few of these (4) did not change from nan's and for those stuborn nan values, we used back fill for these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total wd Missing Values:  0\n"
     ]
    }
   ],
   "source": [
    "time.wd = time.wd.fillna(method = 'ffill')\n",
    "time.wd = time.wd.fillna(method = 'bfill')\n",
    "\n",
    "print('total wd Missing Values: ', time['wd'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WSPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total WSPM Missing Values:  0\n",
      "mean:  1.8608145106091456\n",
      "median:  1.5\n"
     ]
    }
   ],
   "source": [
    "print('total WSPM Missing Values: ', time['WSPM'].isna().sum())\n",
    "print('mean: ', time['WSPM'].mean())\n",
    "print('median: ', time['WSPM'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan Values Filled, check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No         0\n",
       "year       0\n",
       "month      0\n",
       "day        0\n",
       "hour       0\n",
       "PM2.5      0\n",
       "PM10       0\n",
       "SO2        0\n",
       "NO2        0\n",
       "CO         0\n",
       "O3         0\n",
       "TEMP       0\n",
       "PRES       0\n",
       "DEWP       0\n",
       "RAIN       0\n",
       "wd         0\n",
       "WSPM       0\n",
       "station    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine 12 Testing Site Tables\n",
    "We have 12 testing sites with a table for each. \n",
    "So we built a function to automate our concatenation of all of these. \n",
    "For each test site, we loop through these, creating a whole dataframe for visualizations and also a separate loop of this to train test split, interpolate the test then the train using the time method, finally we back fill the stubborn few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time DataFrame, import all as a whole, interpolate by table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the DATA folder, we wil run through all of the individual csv files. \n",
    "\n",
    "path = r'DATA/' \n",
    "allFiles = glob.glob(path + '/*.csv')\n",
    "\n",
    "# Prep the test and train data frames. \n",
    "time = pd.DataFrame()\n",
    "times = []\n",
    "\n",
    "\n",
    "for file_ in allFiles:\n",
    "    \n",
    "    # Read in and set data index to df\n",
    "    df = pd.read_csv(file_,index_col = None,header = 0)\n",
    "    df['Date'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "    df = df.set_index('Date')\n",
    "    df = df.loc[:'2016-12-31 23:00:00']\n",
    "    \n",
    "    # Drop unwanted columns from df into the new data frame time\n",
    "    \n",
    "    time_ = df.drop(columns=['No'], axis = 1)\n",
    "    time_.columns = ['year','month','day','hour','PM2.5','PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN','wd', 'WSPM', 'station']\n",
    "    \n",
    "    \n",
    "    # Interpolate missing values in time, append times list with each iteration\n",
    "    \n",
    "    time_ = time_.interpolate(method = 'time')\n",
    "    time_.wd = time_.wd.fillna(method = 'ffill')\n",
    "    time_.NO2 = time_.NO2.fillna(method = 'bfill')\n",
    "    time_.O3 = time_.O3.fillna(method = 'bfill')\n",
    "    time_.CO = time_.CO.fillna(method = 'bfill')\n",
    "    times.append(time_)\n",
    "    \n",
    "# Make times into the time df, this is the entire df of all 12 tables for visualization purposes\n",
    "time = pd.concat(times)\n",
    "time = time.sort_values(['station', 'Date'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>wd</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>station</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-01 00:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 01:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 02:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 03:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 04:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     year  month  day  hour  PM2.5  PM10   SO2   NO2     CO  \\\n",
       "Date                                                                          \n",
       "2013-03-01 00:00:00  2013      3    1     0    4.0   4.0   4.0   7.0  300.0   \n",
       "2013-03-01 01:00:00  2013      3    1     1    8.0   8.0   4.0   7.0  300.0   \n",
       "2013-03-01 02:00:00  2013      3    1     2    7.0   7.0   5.0  10.0  300.0   \n",
       "2013-03-01 03:00:00  2013      3    1     3    6.0   6.0  11.0  11.0  300.0   \n",
       "2013-03-01 04:00:00  2013      3    1     4    3.0   3.0  12.0  12.0  300.0   \n",
       "\n",
       "                       O3  TEMP    PRES  DEWP  RAIN   wd  WSPM       station  \n",
       "Date                                                                          \n",
       "2013-03-01 00:00:00  77.0  -0.7  1023.0 -18.8   0.0  NNW   4.4  Aotizhongxin  \n",
       "2013-03-01 01:00:00  77.0  -1.1  1023.2 -18.2   0.0    N   4.7  Aotizhongxin  \n",
       "2013-03-01 02:00:00  73.0  -1.1  1023.5 -18.2   0.0  NNW   5.6  Aotizhongxin  \n",
       "2013-03-01 03:00:00  72.0  -1.4  1024.5 -19.4   0.0   NW   3.1  Aotizhongxin  \n",
       "2013-03-01 04:00:00  72.0  -2.0  1025.2 -19.5   0.0    N   2.0  Aotizhongxin  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split Interpolation with 80-20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the DATA folder, we wil run through all of the individual csv files. \n",
    "\n",
    "path = r'DATA/' \n",
    "\n",
    "allFiles = glob.glob(path + '/*.csv')\n",
    "\n",
    "# Prep the test and train data frames.\n",
    "\n",
    "test = pd.DataFrame()\n",
    "train = pd.DataFrame()\n",
    "trains = []\n",
    "tests = []\n",
    "\n",
    "\n",
    "\n",
    "for file_ in allFiles:\n",
    "    \n",
    "    # Read in and set data index. \n",
    "    df = pd.read_csv(file_,index_col = None,header = 0)\n",
    "    df['Date'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "    df = df.set_index('Date')\n",
    "    df = df.loc[:'2016-12-31 23:00:00']\n",
    "    \n",
    "    # Drop unwanted columns \n",
    "    X = df.drop(columns=['PM2.5','year','month','day','hour','No'], axis = 1)\n",
    "    \n",
    "    # Set the target \n",
    "    y = df['PM2.5']\n",
    "    \n",
    "    # Train test split to prevent data leakage\n",
    "    X_train = X[:int(X.shape[0]*0.8)]\n",
    "    X_test = X[int(X.shape[0]*0.8):]\n",
    "    y_train = y[:int(X.shape[0]*0.8)]\n",
    "    y_test = y[int(X.shape[0]*0.8):]\n",
    "    \n",
    "    \n",
    "    train = pd.concat([X_train,y_train], axis = 1, ignore_index=True)\n",
    "    train.columns = ['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN','wd', 'WSPM', 'station','PM2.5']\n",
    "    \n",
    "    test = pd.concat([X_test,y_test], axis = 1, ignore_index=True)\n",
    "    test.columns = ['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN','wd', 'WSPM', 'station','PM2.5']\n",
    "    \n",
    "    # Interpolate the train data and add to the rest of the train list \n",
    "    \n",
    "    train = train.interpolate(method = 'time')\n",
    "    train.wd = train.wd.fillna(method = 'ffill')\n",
    "    train.NO2 = train.NO2.fillna(method = 'bfill')\n",
    "    trains.append(train)\n",
    "    \n",
    "    # Interpolate the test data and add to the rest of the test list\n",
    "    \n",
    "    test = test.interpolate(method = 'time')\n",
    "    test['PM2.5'] = test['PM2.5'].fillna(method = 'bfill')\n",
    "    test.wd = test.wd.fillna(method = 'ffill')\n",
    "    test.O3 = test.O3.fillna(method = 'bfill')\n",
    "    test.CO = test.CO.fillna(method = 'bfill')\n",
    "    tests.append(test)\n",
    "\n",
    "# Take the data fame we made at the top and combine the new data frame we processed here \n",
    "    \n",
    "train = pd.concat(trains)\n",
    "train = train.sort_values(['station', 'Date'])\n",
    "test = pd.concat(tests)\n",
    "test = test.sort_values(['station', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude and Longitude \n",
    "We will add Latidude and Longitude for each testing site for more visualizations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that gets the coordinates of the cities\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_lat_long(city):\n",
    "    country = 'China'\n",
    "    geolocator = Nominatim(user_agent = 'Beijing')\n",
    "    location = geolocator.geocode(city+','+ country)\n",
    "    long = location.longitude\n",
    "    lat = location.latitude\n",
    "    return(lat, long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations are a little off\n",
    "\n",
    "# Aotizhongxin is Aoti Zhongxin\n",
    "# Wanshouxigong is Wan Shou Xin Gong\n",
    "\n",
    "city = ['Aoti Zhongxin','Changping', 'Dingling', 'Dongsi', 'Guanyuan','Gucheng', \n",
    "        'Huairou', 'Nongzhanguan','Shunyi', 'Tiantan','Wanliu', 'Wan Shou Xin Gong']\n",
    "\n",
    "# Use the function to get a list of the coordinates\n",
    "coordinates = []\n",
    "latitude = []\n",
    "longitude = []\n",
    "\n",
    "for i in city: \n",
    "    (get_lat_long(i))\n",
    "    coordinates.append(get_lat_long(i))\n",
    "    \n",
    "for i in range(0,12): \n",
    "    latitude.append(coordinates[i][0])\n",
    "    longitude.append(coordinates[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "time['station_latitude'] = time.station\n",
    "time['station_longitude'] = time.station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c185567faf9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'station_latitude'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'station_longitude'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3636\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3638\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "cond = [[time.station == 'Aotizhongxin'], [time.station == 'Changping'], [time.station == 'Dingling'], \n",
    "        [time.station == 'Dongsi'], [time.station == 'Guanyuan'], [time.station == 'Gucheng'],\n",
    "        [time.station == 'Huairou'], [time.station == 'Nongzhanguan'], [time.station == 'Shunyi'], \n",
    "        [time.station == 'Tiantan'], [time.station == 'Wanliu'], [time.station == 'Wanshouxigong']]\n",
    "\n",
    "\n",
    "\n",
    "time['station_latitude'] = np.select(cond, latitude)\n",
    "time['station_longitude'] = np.select(cond, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'hour', 'PM2.5', 'PM10', 'SO2', 'NO2', 'CO',\n",
       "       'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'wd', 'WSPM', 'station',\n",
       "       'station_latitude', 'station_longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the Time DF and the Train and Test DFs, then proceed to the EDA Notebook\n",
    "The Time DF will be used for visualizations\n",
    "The Train Test Split will be used in Predictions as well as for comparative visuals later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_pickle('PKL/test.pkl')\n",
    "train.to_pickle('PKL/train.pkl')\n",
    "time.to_pickle('PKL/time.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
