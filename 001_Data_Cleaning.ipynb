{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beijing Air-Quality Time Series Project\n",
    "### Data Cleaning Notebook\n",
    "\n",
    "#### by Dolci Sanders and Paul Torres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "\n",
    "Our Data is provided by the UCI Machine Learning Repository\n",
    "Beijing Multi-Site Air-Quality Data\n",
    "\n",
    "Once Read in, we will look at the head and convert our time into data time. \n",
    "\n",
    "We have 12 data sets, one from each reporting site, to concatenate together to get the whole picture of Beijing's Air Quality. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.read_csv('DATA/PRSA_Data_Tiantan_20130301-20170228.csv', index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2013, 2014, 2015, 2016, 2017])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.year.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Time Formatting\n",
    "Crucial to time series, this must be set as the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time['Date'] = pd.to_datetime(time[['year','month','day','hour']])\n",
    "time = time.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35064, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with NaN Values\n",
    "After looking at the data, we wanted to look at null values and figure out what to do with these. \n",
    "\n",
    "We first calculated the mean and median for all of the values. These values varied wildly, and we were concerned they would have a negative effect on the predictions. \n",
    "\n",
    "However, upon further research, other methods have proven more effective in inputing the data for time series such as interpolating time. We elected to try imputing using the interpolate time method. \n",
    "\n",
    "Because we are doing univariate time series, we will not need to worry about the nan values of the other features.\n",
    "--if we did want multivariate, this is still a great imputation option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No            0\n",
       "year          0\n",
       "month         0\n",
       "day           0\n",
       "hour          0\n",
       "PM2.5       677\n",
       "PM10        597\n",
       "SO2        1118\n",
       "NO2         744\n",
       "CO         1126\n",
       "O3          843\n",
       "TEMP         20\n",
       "PRES         20\n",
       "DEWP         20\n",
       "RAIN         20\n",
       "wd           78\n",
       "WSPM         14\n",
       "station       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = time.interpolate(method='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PM2.5 (Target Variable)\n",
    "After interpolating using the time method, you can see this value now has no missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total PM2.5 Missing Values:  0\n",
      "mean:  82.03309662331738\n",
      "median:  58.0\n"
     ]
    }
   ],
   "source": [
    "print('total PM2.5 Missing Values: ', time['PM2.5'].isna().sum())\n",
    "print('mean: ', time['PM2.5'].mean())\n",
    "print('median: ', time['PM2.5'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine 12 Testing Site Tables\n",
    "We have 12 testing sites with a table for each. \n",
    "So we built a function to automate our concatenation of all of these. \n",
    "For each test site, we loop through these, creating a whole dataframe for visualizations and also a separate loop of this to train test split, interpolate the test then the train using the time method.\n",
    "\n",
    "Each of these will interpolate each table separately as shown with one table above, and then concatenate them for the most accurate results. \n",
    "\n",
    "The first will keep the whole data set, in case we want to do some additional visualizations. \n",
    "The second will be our test train split, which will be Date and PM2.5 values only. \n",
    "Thirdly we will leave out a set with stations still attached for further use at a later date. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time DataFrame, import all as a whole, interpolate by table\n",
    "Here we have the full data frame, not split with all the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>WSPM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-01</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>7.326389</td>\n",
       "      <td>12.203125</td>\n",
       "      <td>9.182292</td>\n",
       "      <td>19.990602</td>\n",
       "      <td>404.861111</td>\n",
       "      <td>68.244792</td>\n",
       "      <td>1.173958</td>\n",
       "      <td>1026.569792</td>\n",
       "      <td>-20.148611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.917708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-02</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>31.475694</td>\n",
       "      <td>40.506944</td>\n",
       "      <td>31.757878</td>\n",
       "      <td>56.303763</td>\n",
       "      <td>939.715278</td>\n",
       "      <td>32.601112</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>1026.552778</td>\n",
       "      <td>-16.271875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.145139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-03</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>79.291667</td>\n",
       "      <td>111.104167</td>\n",
       "      <td>49.286458</td>\n",
       "      <td>75.661458</td>\n",
       "      <td>1771.142361</td>\n",
       "      <td>22.788194</td>\n",
       "      <td>5.257292</td>\n",
       "      <td>1014.215278</td>\n",
       "      <td>-12.325347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.463194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-04</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>21.768569</td>\n",
       "      <td>40.359375</td>\n",
       "      <td>18.818692</td>\n",
       "      <td>40.877151</td>\n",
       "      <td>726.340278</td>\n",
       "      <td>56.833333</td>\n",
       "      <td>9.610764</td>\n",
       "      <td>1017.263194</td>\n",
       "      <td>-12.773264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.227083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-05</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>125.352959</td>\n",
       "      <td>159.236111</td>\n",
       "      <td>70.756944</td>\n",
       "      <td>103.337674</td>\n",
       "      <td>2021.434028</td>\n",
       "      <td>79.527778</td>\n",
       "      <td>6.630556</td>\n",
       "      <td>1010.551389</td>\n",
       "      <td>-7.916319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              year  month  day  hour       PM2.5        PM10        SO2  \\\n",
       "Date                                                                      \n",
       "2013-03-01  2013.0    3.0  1.0  11.5    7.326389   12.203125   9.182292   \n",
       "2013-03-02  2013.0    3.0  2.0  11.5   31.475694   40.506944  31.757878   \n",
       "2013-03-03  2013.0    3.0  3.0  11.5   79.291667  111.104167  49.286458   \n",
       "2013-03-04  2013.0    3.0  4.0  11.5   21.768569   40.359375  18.818692   \n",
       "2013-03-05  2013.0    3.0  5.0  11.5  125.352959  159.236111  70.756944   \n",
       "\n",
       "                   NO2           CO         O3      TEMP         PRES  \\\n",
       "Date                                                                    \n",
       "2013-03-01   19.990602   404.861111  68.244792  1.173958  1026.569792   \n",
       "2013-03-02   56.303763   939.715278  32.601112  0.260417  1026.552778   \n",
       "2013-03-03   75.661458  1771.142361  22.788194  5.257292  1014.215278   \n",
       "2013-03-04   40.877151   726.340278  56.833333  9.610764  1017.263194   \n",
       "2013-03-05  103.337674  2021.434028  79.527778  6.630556  1010.551389   \n",
       "\n",
       "                 DEWP  RAIN      WSPM  \n",
       "Date                                   \n",
       "2013-03-01 -20.148611   0.0  2.917708  \n",
       "2013-03-02 -16.271875   0.0  1.145139  \n",
       "2013-03-03 -12.325347   0.0  1.463194  \n",
       "2013-03-04 -12.773264   0.0  2.227083  \n",
       "2013-03-05  -7.916319   0.0  0.992014  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the DATA folder, we wil run through all of the individual csv files.\n",
    "path = r'DATA/'\n",
    "allFiles = glob.glob(path + '/*.csv')\n",
    "\n",
    "# Prep the test and train data frames.\n",
    "time = pd.DataFrame()\n",
    "times = []\n",
    "\n",
    "\n",
    "for file_ in allFiles:\n",
    "    \n",
    "    # Read in and set data index.\n",
    "    df = pd.read_csv(file_,index_col = None,header = 0)\n",
    "    df['Date'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "    df = df.set_index('Date')\n",
    "    df = df.loc[:'2016-12-31 23:00:00']\n",
    "    \n",
    "    \n",
    "    # Univariate time series, drop the other features\n",
    "    df = df.drop(columns =['No'])\n",
    "    \n",
    "\n",
    "    # Interpolate the train data and add to the rest of the train list\n",
    "    df = df.interpolate(method = 'time')\n",
    "    times.append(df)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Take the data frame we made at the top and combine the new data frame we processed here\n",
    "time = pd.concat(times)\n",
    "time = time.sort_values(['station','Date'])\n",
    "time = time.resample('D').mean()\n",
    "time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Train Test Split Interpolation with 80-20 Split\n",
    "\n",
    "Because time series needs specific splits of the train and test, we have done this in the function. \n",
    "We have elected to use an 80-20 split with the last 20 being the test set for comparing the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the DATA folder, we wil run through all of the individual csv files. \n",
    "\n",
    "path = r'DATA/'\n",
    "allFiles = glob.glob(path + '/*.csv')\n",
    "\n",
    "# Prep the test and train data frames.\n",
    "\n",
    "test = pd.DataFrame()\n",
    "train = pd.DataFrame()\n",
    "\n",
    "trains = []\n",
    "tests = []\n",
    "\n",
    "\n",
    "for file_ in allFiles:\n",
    "    \n",
    "    # Read in and set data index. \n",
    "    df = pd.read_csv(file_,index_col = None,header = 0)\n",
    "    df['Date'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "    df = df.set_index('Date')\n",
    "    #Limits to end 2016 \n",
    "    df = df.loc[:'2016-12-31 23:00:00']\n",
    "    df = df.drop(columns =['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', \n",
    "                           'DEWP', 'RAIN','wd', 'WSPM','No','year','month','day','hour'])\n",
    "\n",
    "    df = df.resample('D').mean()\n",
    "    \n",
    "    # Drop unwanted columns, in univariate we should only have the date and the target. \n",
    "    \n",
    "    # Train test split to prevent data leakage\n",
    "    \n",
    "    train = df[:int(df.shape[0]*0.8)]\n",
    "    test = df[int(df.shape[0]*0.8):]\n",
    "    \n",
    "    \n",
    "    # Interpolate the train data and add to the rest of the train list \n",
    "    \n",
    "    train = train.interpolate(method = 'time')\n",
    "    train['PM2.5'] = train['PM2.5'].fillna(method = 'bfill')\n",
    "    trains.append(train)\n",
    "    \n",
    "    # Interpolate the test data and add to the rest of the test list\n",
    "    \n",
    "    test = test.interpolate(method = 'time')\n",
    "    test['PM2.5'] = test['PM2.5'].fillna(method = 'bfill')\n",
    "    tests.append(test)\n",
    "    \n",
    "    \n",
    "# Take the data fame we made at the top and combine the new data frame we processed here \n",
    "    \n",
    "train = pd.concat(trains)\n",
    "test = pd.concat(tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.resample('D').mean()\n",
    "train = train.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1121, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Stations not Concatenated, callable from a list\n",
    "This will allow us to predict on individual sites (stations) if time allows, or after this project, we would like to investigate each site further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the DATA folder, we wil run through all of the individual csv files. \n",
    "\n",
    "path = r'DATA/' \n",
    "allFiles = glob.glob(path + '/*.csv')\n",
    "\n",
    "# Prep the test and train data frames.\n",
    "\n",
    "test = pd.DataFrame()\n",
    "train = pd.DataFrame()\n",
    "all_trains = []\n",
    "all_tests = []\n",
    "\n",
    "for file_ in allFiles:\n",
    "    \n",
    "    # Read in and set data index. \n",
    "    df = pd.read_csv(file_,index_col = None,header = 0)\n",
    "    df['Date'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "    df = df.set_index('Date')\n",
    "    #Limits to end 2016 \n",
    "    df = df.loc[:'2016-12-31 23:00:00']\n",
    "    \n",
    "    # Drop unwanted columns, in univariate we should only have the date and the target. \n",
    "    df = df.drop(columns=['year','month','day','hour','No', \n",
    "                         'PM10', 'SO2', 'NO2', 'CO', 'O3', \n",
    "                         'TEMP', 'PRES', 'DEWP', 'RAIN','wd', 'WSPM'], axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Train test split to prevent data leakage\n",
    "    train = df[:int(df.shape[0]*0.8)]\n",
    "    test = df[int(df.shape[0]*0.8):]\n",
    "     \n",
    "    \n",
    "    # Interpolate the train data and add to the rest of the train list \n",
    "    \n",
    "    train = train.interpolate(method = 'time')\n",
    "    all_trains.append(train)\n",
    "    \n",
    "    # Interpolate the test data and add to the rest of the test list\n",
    "    \n",
    "    test = test.interpolate(method = 'time')\n",
    "    test['PM2.5'] = test['PM2.5'].fillna(method = 'bfill')\n",
    "    all_tests.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gucheng = all_tests[0]\n",
    "Huairou = all_tests[1]\n",
    "Tiantan = all_tests[2]\n",
    "Changping = all_tests[3]\n",
    "Guanyuan = all_tests[4]\n",
    "Nongzhanguan = all_tests[5]\n",
    "Wanliu = all_tests[6]\n",
    "Dongsi = all_tests[7]\n",
    "Wanshouxigong = all_tests[8]\n",
    "Aotizhongxin = all_tests[9]\n",
    "Dingling = all_tests[10]\n",
    "Shunyi = all_tests[11]\n",
    "\n",
    "trainGucheng = all_trains[0]\n",
    "trainHuairou = all_trains[1]\n",
    "trainTiantan = all_trains[2]\n",
    "trainChangping = all_trains[3]\n",
    "trainGuanyuan = all_trains[4]\n",
    "trainNongzhanguan = all_trains[5]\n",
    "trainWanliu = all_trains[6]\n",
    "trainDongsi = all_trains[7]\n",
    "trainWanshouxigong = all_trains[8]\n",
    "trainAotizhongxin = all_trains[9]\n",
    "trainDingling = all_trains[10]\n",
    "trainShunyi = all_trains[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the Time DF and the Train and Test DFs, then proceed to the EDA Notebook\n",
    "The Time DF will be used for visualizations.\n",
    "The Train Test Split will be used in Predictions as well as for comparative visuals later on. \n",
    "The sites will be used at a later date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_pickle('PKL/test.pkl')\n",
    "train.to_pickle('PKL/train.pkl')\n",
    "time.to_pickle('PKL/time.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Station Sites \n",
    "\n",
    "Aotizhongxin.to_pickle('PKL/Aotizhongxin.pkl')\n",
    "Changping.to_pickle('PKL/Changping.pkl')\n",
    "Dingling.to_pickle('PKL/Dingling.pkl')\n",
    "Dongsi.to_pickle('PKL/Dongsi.pkl')\n",
    "Guanyuan.to_pickle('PKL/Guanyuan.pkl')\n",
    "Gucheng.to_pickle('PKL/Gucheng.pkl')\n",
    "Huairou.to_pickle('PKL/Huairou.pkl')\n",
    "Nongzhanguan.to_pickle('PKL/Nongzhanguan.pkl')\n",
    "Shunyi.to_pickle('PKL/Shunyi.pkl')\n",
    "Tiantan.to_pickle('PKL/Tiantan.pkl')\n",
    "Wanliu.to_pickle('PKL/Wanliu.pkl')\n",
    "Wanshouxigong.to_pickle('PKL/Wanshouxigong.pkl')\n",
    "\n",
    "trainGucheng.to_pickle('PKL/trainGucheng.pkl')\n",
    "trainHuairou.to_pickle('PKL/trainHuairou.pkl')\n",
    "trainTiantan.to_pickle('PKL/trainTiantan.pkl')\n",
    "trainChangping.to_pickle('PKL/trainChangping.pkl')\n",
    "trainGuanyuan.to_pickle('PKL/trainGuanyuan.pkl')\n",
    "trainNongzhanguan.to_pickle('PKL/trainNongzhanguan.pkl')\n",
    "trainWanliu.to_pickle('PKL/trainWanliu.pkl')\n",
    "trainDongsi.to_pickle('PKL/trainDongsi.pkl')\n",
    "trainWanshouxigong.to_pickle('PKL/trainWanshouxigong.pkl')\n",
    "trainAotizhongxin.to_pickle('PKL/trainAotizhongxin.pkl')\n",
    "trainDingling.to_pickle('PKL/trainDingling.pkl')\n",
    "trainShunyi.to_pickle('PKL/trainShunyi.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
