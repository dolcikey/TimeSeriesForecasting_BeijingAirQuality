{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beijing Air-Quality Time Series Project\n",
    "### Data Cleaning Notebook\n",
    "\n",
    "by Dolci Sanders and Paul Torres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "\n",
    "Our Data is provided by the UCI Machine Learning Repository\n",
    "Beijing Multi-Site Air-Quality Data\n",
    "\n",
    "Once Read in, we will look at the head and convert our time into data time. \n",
    "\n",
    "We have 12 data sets, one from each reporting site, to concatenate together to get the whole picture of Beijing's Air Quality. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.read_csv('DATA/PRSA_Data_Tiantan_20130301-20170228.csv', index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2013, 2014, 2015, 2016, 2017])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.year.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Time Formatting\n",
    "Crucial to time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time['Date'] = pd.to_datetime(time[['year','month','day','hour']])\n",
    "time = time.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35064, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Nan Values\n",
    "After looking at the data, we wanted to look at null values and figure out what to do with these. \n",
    "\n",
    "We first calculated the mean and media for all of the values. These values varied wildly and we were concerned they would have a negative effect on the predictions. \n",
    "\n",
    "However, upon further research, other methods have proven more effective in inputing the data for time series such as Interpolating Time, however because we have hourly time this was not the best method. We elected to try imputing using the InterpolateLinear method. \n",
    "\n",
    "Because we are doing univariate time series, however, we will not need to worry about the other features at this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No            0\n",
       "year          0\n",
       "month         0\n",
       "day           0\n",
       "hour          0\n",
       "PM2.5       677\n",
       "PM10        597\n",
       "SO2        1118\n",
       "NO2         744\n",
       "CO         1126\n",
       "O3          843\n",
       "TEMP         20\n",
       "PRES         20\n",
       "DEWP         20\n",
       "RAIN         20\n",
       "wd           78\n",
       "WSPM         14\n",
       "station       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = time.interpolate(method='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PM2.5 (Target Variable)\n",
    "You can see this value now has no missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total PM2.5 Missing Values:  0\n",
      "mean:  82.03309662331738\n",
      "median:  58.0\n"
     ]
    }
   ],
   "source": [
    "print('total PM2.5 Missing Values: ', time['PM2.5'].isna().sum())\n",
    "print('mean: ', time['PM2.5'].mean())\n",
    "print('median: ', time['PM2.5'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine 12 Testing Site Tables\n",
    "We have 12 testing sites with a table for each. \n",
    "So we built a function to automate our concatenation of all of these. \n",
    "For each test site, we loop through these, creating a whole dataframe for visualizations and also a separate loop of this to train test split, interpolate the test then the train using the time method, finally we back fill the stubborn few.\n",
    "\n",
    "Each of these will interpolate each table separately as shown with one table above, and then concatenate them for the most accurate results. \n",
    "\n",
    "The first will keep the whole data set, in case we want to do some additional visualizations. \n",
    "The second will be our test train split, which will be Date and PM2.5 values only. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time DataFrame, import all as a whole, interpolate by table\n",
    "Here we have the full data frame, not split with all the columns. This is not resampled to the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>wd</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>station</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-01 00:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 01:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 02:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 03:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 04:00:00</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     year  month  day  hour  PM2.5  PM10   SO2   NO2     CO  \\\n",
       "Date                                                                          \n",
       "2013-03-01 00:00:00  2013      3    1     0    4.0   4.0   4.0   7.0  300.0   \n",
       "2013-03-01 01:00:00  2013      3    1     1    8.0   8.0   4.0   7.0  300.0   \n",
       "2013-03-01 02:00:00  2013      3    1     2    7.0   7.0   5.0  10.0  300.0   \n",
       "2013-03-01 03:00:00  2013      3    1     3    6.0   6.0  11.0  11.0  300.0   \n",
       "2013-03-01 04:00:00  2013      3    1     4    3.0   3.0  12.0  12.0  300.0   \n",
       "\n",
       "                       O3  TEMP    PRES  DEWP  RAIN   wd  WSPM       station  \n",
       "Date                                                                          \n",
       "2013-03-01 00:00:00  77.0  -0.7  1023.0 -18.8   0.0  NNW   4.4  Aotizhongxin  \n",
       "2013-03-01 01:00:00  77.0  -1.1  1023.2 -18.2   0.0    N   4.7  Aotizhongxin  \n",
       "2013-03-01 02:00:00  73.0  -1.1  1023.5 -18.2   0.0  NNW   5.6  Aotizhongxin  \n",
       "2013-03-01 03:00:00  72.0  -1.4  1024.5 -19.4   0.0   NW   3.1  Aotizhongxin  \n",
       "2013-03-01 04:00:00  72.0  -2.0  1025.2 -19.5   0.0    N   2.0  Aotizhongxin  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the DATA folder, we wil run through all of the individual csv files.\n",
    "path = r'DATA/'\n",
    "allFiles = glob.glob(path + '/*.csv')\n",
    "\n",
    "# Prep the test and train data frames.\n",
    "time = pd.DataFrame()\n",
    "times = []\n",
    "\n",
    "\n",
    "for file_ in allFiles:\n",
    "    \n",
    "    # Read in and set data index.\n",
    "    df = pd.read_csv(file_,index_col = None,header = 0)\n",
    "    df['Date'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "    df = df.set_index('Date')\n",
    "    df = df.loc[:'2016-12-31 23:00:00']\n",
    "    \n",
    "    # Univariate time series, drop the other features\n",
    "    df = df.drop(columns =['No'])\n",
    "    \n",
    "\n",
    "    # Interpolate the train data and add to the rest of the train list\n",
    "    df = df.interpolate(method = 'time')\n",
    "    times.append(df)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Take the data frame we made at the top and combine the new data frame we processed here\n",
    "time = pd.concat(times)\n",
    "time = time.sort_values(['station','Date'])\n",
    "time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Train Test Split Interpolation with 80-20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the DATA folder, we wil run through all of the individual csv files. \n",
    "\n",
    "path = r'DATA/' \n",
    "allFiles = glob.glob(path + '/*.csv')\n",
    "\n",
    "# Prep the test and train data frames.\n",
    "\n",
    "test = pd.DataFrame()\n",
    "train = pd.DataFrame()\n",
    "trains = []\n",
    "tests = []\n",
    "\n",
    "for file_ in allFiles:\n",
    "    \n",
    "    # Read in and set data index. \n",
    "    df = pd.read_csv(file_,index_col = None,header = 0)\n",
    "    df['Date'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "    df = df.set_index('Date')\n",
    "    #Limits to end 2016 \n",
    "    df = df.loc[:'2016-12-31 23:00:00']\n",
    "    \n",
    "    # Drop unwanted columns, in univariate we should only have the date and the target. \n",
    "    df = df.drop(columns=['year','month','day','hour','No', \n",
    "                         'PM10', 'SO2', 'NO2', 'CO', 'O3', \n",
    "                         'TEMP', 'PRES', 'DEWP', 'RAIN','wd', 'WSPM'], axis = 1)\n",
    "    \n",
    "    \n",
    "    df = df.resample('D').mean()\n",
    "    \n",
    "    # Train test split to prevent data leakage\n",
    "    train = df[:int(df.shape[0]*0.8)]\n",
    "    test = df[int(df.shape[0]*0.8):]\n",
    "     \n",
    "    \n",
    "    # Interpolate the train data and add to the rest of the train list \n",
    "    \n",
    "    train = train.interpolate(method = 'time')\n",
    "    trains.append(train)\n",
    "    \n",
    "    # Interpolate the test data and add to the rest of the test list\n",
    "    \n",
    "    test = test.interpolate(method = 'time')\n",
    "    test['PM2.5'] = test['PM2.5'].fillna(method = 'bfill')\n",
    "    tests.append(test)\n",
    "    \n",
    "     \n",
    "    \n",
    "\n",
    "# Take the data fame we made at the top and combine the new data frame we processed here \n",
    "    \n",
    "train = pd.concat(trains)\n",
    "train = train.sort_values(['Date'])\n",
    "test = pd.concat(tests)\n",
    "test = test.sort_values(['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Stations not Concatenated, callable from a list\n",
    "This will allow us to predict on individual sites (stations) if time allows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the DATA folder, we wil run through all of the individual csv files. \n",
    "\n",
    "path = r'DATA/' \n",
    "allFiles = glob.glob(path + '/*.csv')\n",
    "\n",
    "# Prep the test and train data frames.\n",
    "\n",
    "test = pd.DataFrame()\n",
    "train = pd.DataFrame()\n",
    "all_trains = []\n",
    "all_tests = []\n",
    "\n",
    "for file_ in allFiles:\n",
    "    \n",
    "    # Read in and set data index. \n",
    "    df = pd.read_csv(file_,index_col = None,header = 0)\n",
    "    df['Date'] = pd.to_datetime(df[['year','month','day','hour']])\n",
    "    df = df.set_index('Date')\n",
    "    #Limits to end 2016 \n",
    "    df = df.loc[:'2016-12-31 23:00:00']\n",
    "    \n",
    "    # Drop unwanted columns, in univariate we should only have the date and the target. \n",
    "    df = df.drop(columns=['year','month','day','hour','No', \n",
    "                         'PM10', 'SO2', 'NO2', 'CO', 'O3', \n",
    "                         'TEMP', 'PRES', 'DEWP', 'RAIN','wd', 'WSPM'], axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Train test split to prevent data leakage\n",
    "    train = df[:int(df.shape[0]*0.8)]\n",
    "    test = df[int(df.shape[0]*0.8):]\n",
    "     \n",
    "    \n",
    "    # Interpolate the train data and add to the rest of the train list \n",
    "    \n",
    "    train = train.interpolate(method = 'time')\n",
    "    all_trains.append(train)\n",
    "    \n",
    "    # Interpolate the test data and add to the rest of the test list\n",
    "    \n",
    "    test = test.interpolate(method = 'time')\n",
    "    test['PM2.5'] = test['PM2.5'].fillna(method = 'bfill')\n",
    "    all_tests.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gucheng = all_tests[0]\n",
    "Huairou = all_tests[1]\n",
    "Tiantan = all_tests[2]\n",
    "Changping = all_tests[3]\n",
    "Guanyuan = all_tests[4]\n",
    "Nongzhanguan = all_tests[5]\n",
    "Wanliu = all_tests[6]\n",
    "Dongsi = all_tests[7]\n",
    "Wanshouxigong = all_tests[8]\n",
    "Aotizhongxin = all_tests[9]\n",
    "Dingling = all_tests[10]\n",
    "Shunyi = all_tests[11]\n",
    "\n",
    "trainGucheng = all_trains[0]\n",
    "trainHuairou = all_trains[1]\n",
    "trainTiantan = all_trains[2]\n",
    "trainChangping = all_trains[3]\n",
    "trainGuanyuan = all_trains[4]\n",
    "trainNongzhanguan = all_trains[5]\n",
    "trainWanliu = all_trains[6]\n",
    "trainDongsi = all_trains[7]\n",
    "trainWanshouxigong = all_trains[8]\n",
    "trainAotizhongxin = all_trains[9]\n",
    "trainDingling = all_trains[10]\n",
    "trainShunyi = all_trains[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the Time DF and the Train and Test DFs, then proceed to the EDA Notebook\n",
    "The Time DF will be used for visualizations\n",
    "The Train Test Split will be used in Predictions as well as for comparative visuals later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test.to_pickle('PKL/test.pkl')\n",
    "train.to_pickle('PKL/train.pkl')\n",
    "time.to_pickle('PKL/time.pkl')\n",
    "\n",
    "# Station Sites \n",
    "\n",
    "Aotizhongxin.to_pickle('PKL/Aotizhongxin.pkl')\n",
    "Changping.to_pickle('PKL/Changping.pkl')\n",
    "Dingling.to_pickle('PKL/Dingling.pkl')\n",
    "Dongsi.to_pickle('PKL/Dongsi.pkl')\n",
    "Guanyuan.to_pickle('PKL/Guanyuan.pkl')\n",
    "Gucheng.to_pickle('PKL/Gucheng.pkl')\n",
    "Huairou.to_pickle('PKL/Huairou.pkl')\n",
    "Nongzhanguan.to_pickle('PKL/Nongzhanguan.pkl')\n",
    "Shunyi.to_pickle('PKL/Shunyi.pkl')\n",
    "Tiantan.to_pickle('PKL/Tiantan.pkl')\n",
    "Wanliu.to_pickle('PKL/Wanliu.pkl')\n",
    "Wanshouxigong.to_pickle('PKL/Wanshouxigong.pkl')\n",
    "\n",
    "trainGucheng.to_pickle('PKL/trainGucheng.pkl')\n",
    "trainHuairou.to_pickle('PKL/trainHuairou.pkl')\n",
    "trainTiantan.to_pickle('PKL/trainTiantan.pkl')\n",
    "trainChangping.to_pickle('PKL/trainChangping.pkl')\n",
    "trainGuanyuan.to_pickle('PKL/trainGuanyuan.pkl')\n",
    "trainNongzhanguan.to_pickle('PKL/trainNongzhanguan.pkl')\n",
    "trainWanliu.to_pickle('PKL/trainWanliu.pkl')\n",
    "trainDongsi.to_pickle('PKL/trainDongsi.pkl')\n",
    "trainWanshouxigong.to_pickle('PKL/trainWanshouxigong.pkl')\n",
    "trainAotizhongxin.to_pickle('PKL/trainAotizhongxin.pkl')\n",
    "trainDingling.to_pickle('PKL/trainDingling.pkl')\n",
    "trainShunyi.to_pickle('PKL/trainShunyi.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
